1. Create notebook.ipynb
2. Create src\data_collection.py 




                                          ________________ Comment __________

1. python src/data_collection.py
2. dvc init
3. dvc stage add -n data_collection -d src/data_collection.py -o data/raw python src/data_collection.py
4. dvc repro
5. dvc dag
6. dvc stage add -n data_preprocessing -d src/data_preprocessing.py -d data/raw -o data/processed python src/data_preprocessing.py
7. dvc repro
8. dvc dag













__________________________________________________________________________________________________________

sample code 
                                                       1. data_collection

import numpy as np
import pandas as pd
import sklearn
from  sklearn.model_selection import train_test_split
import yaml
import os
import logging



url = "https://raw.githubusercontent.com/SandeepSuthar169/Datasets/main/bank.csv"
data = pd.read_csv(url)

train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

data_path = os.path.join('data', 'raw')

os.makedirs(data_path)

train_data.to_csv(os.path.join(data_path, 'train.csv'), index=False)
test_data.to_csv(os.path.join(data_path, 'test.csv'), index =False)

___________________________________________________________________________________________________________

                                                      2. data_preprocessing

import pandas as pd
import numpy as np
import os

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline

train_data = pd.read_csv("./data/raw/train.csv")
test_data = pd.read_csv("./data/raw/test.csv")


def columns_label_encoding(df):
    df['marital'] = df['marital'].replace({
    'divorced': 1,
    'single': 2,
    'married': 3
    })
    df['marital'] = df['marital'].astype(int)

    df['job'] = df['job'].replace({
    'unknown': 1,
    'unemployed': 1,
    'housemaid': 2,
    'student': 2,
    'self-employed': 3,
    'retired': 4,
    'services': 4,
    'admin.': 5,
    'technician': 5,
    'blue-collar': 5,
    'management': 6,
    'entrepreneur': 6   
    })
    df['job'] = df['job'].astype(int)

    df['poutcome'] = df['poutcome'].replace({
    'unknown': 1,
    'other': 1,
    'failure': 2,
    'success': 2    
    })
    df['poutcome'] = df['poutcome'].astype(int)

    df['education'] = df['education'].replace({
    'unknown': 1,
    'primary': 2,
    'tertiary': 3,
    'secondary': 4
    })
    df['education'] = df['education'].astype(int)

    df['contact'] = df['contact'].replace({
    'unknown': 1,
    'cellular': 2,
    'telephone': 3   
    })
    df['contact'] = df['contact'].astype(int)
    return df

train_pro_data = columns_label_encoding(train_data)
test_pro_data = columns_label_encoding(test_data)

data_path = os.path.join("data", "processed")
os.makedirs(data_path)

train_pro_data.to_csv(os.path.join(data_path, "train_processed.csv"), index=False)
test_pro_data.to_csv(os.path.join(data_path, "test_processed.csv"), index= False)

____________________________________________________________________________________________________________

                                               3. model_building

import numpy as np
import pandas as pd
import os
import pickle
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

train_data = pd.read_csv("./data/processed/train_processed.csv")


X_train = train_data.drop(columns=['deposit'])
y_train = train_data['deposit']

#deposit, month, loan, housing, default
prepro= ColumnTransformer(transformers=[
        ('one', OneHotEncoder(), ['default', 'housing', 'loan', 'month'])
    ], 
        remainder='passthrough'
    )


pipe = Pipeline(steps =[
        ('prepro', prepro),
        ("classi", RandomForestClassifier())
    ])
    
 
pipe.fit(X_train, y_train)

pickle.dump(pipe, open('model.pkl', 'wb'))

___________________________________________________________________________________________________________
                                        4. model_evaluation.py

                                        import numpy as np
import pandas as pd
import json
import os
import pickle

from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

test_data = pd.read_csv("./data/processed/test_processed.csv")

X_test = test_data.drop(columns=['deposit'])
y_test = test_data['deposit']

pipe = pickle.load(open("model.pkl", "rb"))

y_pred = pipe.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
#precision = precision_score(y_test, y_pred)
#recall = recall_score(y_test,y_pred)
#f1 = f1_score(y_test, y_pred)

metrics = {
    'accuracy':accuracy,
    #'precision':precision,
    #'recall':recall,
    #'f1':f1
}

with open('metrics.json', 'w') as file:
    json.dump(metrics, file, indent=4)
